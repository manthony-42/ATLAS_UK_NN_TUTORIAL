{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "DNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ps2xK8_3bfvr",
        "iX6XQouebRLQ"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS5RSOR8Q3FP"
      },
      "source": [
        "# Welcome to the ATLAS UK (Jan. 2021) ML tutorial!    \r\n",
        "## This tutorial will attempt to guide you through a short analysis chain. The general outline of this tutorial is as follows:\r\n",
        "\r\n",
        "1. Setup the workbook environment and download some ATLAS Open Data samples. \r\n",
        "2. Implement some basic selections and make some plots of the kinematic variables.\r\n",
        "3. Train a deep neural network to discriminate between the signal process of interest and the major background sample.\r\n",
        "4. Test the significance of the discrimination using a cut on the output of the DNN.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIwBfFf5Qsnx"
      },
      "source": [
        "## 0. Colab workbook basics\n",
        "#### *Running cells in Google Colab*: \n",
        "1.   Run each cell individually (or stop/restart them) by clicking on the arrow icon on each code cell. Cells marked with a number have been already run.\n",
        "2.   `Runtime` menu allows you to clear all cells, run all cells etc.\n",
        "\n",
        "#### *Adding cells in Google Colab*:\n",
        "Both cells and textboxes can be appended to google colab either by clicking the relevant section and then clicking the \"+ Code\" button or \"+Text\"  option above, or by hovering over the bottom of the relevant section/block to see the same options. \n",
        "\n",
        "#### *Saving the notebook*\n",
        "You can save a copy with the updates you make into your own Google Drive area (using \"Copy to Drive\" or File/Save a copy in Drive\"), and there is github integration (unfortunately NOT Gitlab) under the \"File/Save to Github\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPvFvUGVHvg"
      },
      "source": [
        "### 1. Getting the workbook environment set up!\r\n",
        "This tutorial makes use of modern HEP software libraries, such as [Uproot4](https://uproot.readthedocs.io/en/latest/), while also leverging tools widely used in data science, such as Pandas, numpy and sklearn. Uproot4 enables us to use ROOT files as inputs to this tutorial, without needing an install of ROOT to hand!    \r\n",
        "To get started, set:\r\n",
        "\r\n",
        "```\r\n",
        "first_run = True\r\n",
        "```\r\n",
        "\r\n",
        "After doing this, run the first two cells. Every time you open this workbook on a new instance of Google Colab, you will need to run this step to install the required packages. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTKfrXbfhMCc"
      },
      "source": [
        "first_run = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P--kTtVvccp5"
      },
      "source": [
        "if first_run:\n",
        "  print('--- INSTALLING PACKAGES ---')\n",
        "  !python3 -m pip install --upgrade pip\n",
        "  !python3 -m pip install tensorflow==2.2.0\n",
        "  !python3 -m pip install uproot4\n",
        "  !python3 -m pip install awkward1\n",
        "else:\n",
        "  print('--- SKIPPING INSTALL ---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzyoXSPjXBa-"
      },
      "source": [
        "In addition, we should now download the ATLAS Open Data files that we will use for this tutorial. Again, we should do this when we first run the workbook on an instance of Google Colab. A lot of helpful info about the available ATLAS Open Data samples can be found here: [ATLAS Open Data portal](http://opendata.atlas.cern/release/2020/documentation/).    \r\n",
        "You may have noticed that the previous cell and this cell use shell commands. To use standard shell commands, start the line with an exclamation mark. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XMn8iYYXGJn"
      },
      "source": [
        "if first_run:\r\n",
        "  !rm *.root*\r\n",
        "  !wget https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/1largeRjet1lep/MC/mc_387163.TT_directTT_600_1.1largeRjet1lep.root\r\n",
        "  !wget https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/1largeRjet1lep/MC/mc_410000.ttbar_lep.1largeRjet1lep.root\r\n",
        "  !ls\r\n",
        "first_run=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbnG2IDJWylG"
      },
      "source": [
        "Finally, we will import the relevant modules and functions. We will use Keras, a library which interfaces to TensorFlow, for our deep neural network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eynCzutzccqE"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.random import set_seed # import set_seed function for TensorFlow\n",
        "from numpy.random import seed\n",
        "seed_value=56\n",
        "set_seed(seed_value) # set TensorFlow random seed\n",
        "seed(seed_value) # set numpy random seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0iANR_Z6Kll"
      },
      "source": [
        "In addition, we will use some sklearn features when preparing our datasets for training and testing. We will make use of Pandas to store the information we extract from the ROOT files, and use matplotlib to make all plots in this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqN7g4rQccqF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import uproot4\n",
        "import numpy as np\n",
        "import progressbar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl1nMyd7eL-U"
      },
      "source": [
        "### 2. Taking a look at some ROOT files\r\n",
        "As mentioned earlier, we will use ATLAS Open Data samples in this notebook. Using two simulated samples, we will look at discriminating a SUSY process from its largest background, top-pair production. \r\n",
        "\r\n",
        "![]()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63qOZmxWeU5S"
      },
      "source": [
        "stop_file = uproot4.open(\"mc_387163.TT_directTT_600_1.1largeRjet1lep.root\")\r\n",
        "top_file = uproot4.open(\"mc_410000.ttbar_lep.1largeRjet1lep.root\")\r\n",
        "stop_tree = stop_file['mini']\r\n",
        "top_tree = top_file[\"mini\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGidfzTwLgHT"
      },
      "source": [
        "If you would like to see the contents of the input ROOT files, set:\r\n",
        "```\r\n",
        "print_tree = True\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCNaq1ixgTmq"
      },
      "source": [
        "print_tree = False\r\n",
        "if print_tree:\r\n",
        "  stop_tree.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6llbL0MKLvue"
      },
      "source": [
        "To get started, we will apply a loose selection to events, requiring that each event has at least two jets (R=0.4), has exactly one lepton and passes one of the single lepton triggers, and also has exactly one large-R jet.    \r\n",
        "To save time, we will use only 20000 events from the top-pair sample. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypSjX-OGNuVN"
      },
      "source": [
        "cuts = '(jet_n >= 2) * (lep_n == 1) * (trigE + trigM == 1) * (largeRjet_n == 1)' \r\n",
        "events_limit = 20000 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyiimkKfksHg"
      },
      "source": [
        "def prepare_dataset( tree ):\r\n",
        "\r\n",
        "  # Retrieve branches from trees in awkward array\r\n",
        "  vars = ['met_et','met_phi','jet_pt',\r\n",
        "          'jet_phi','jet_eta','lep_pt',\r\n",
        "          'lep_eta','lep_phi','largeRjet_pt',\r\n",
        "          'largeRjet_eta','largeRjet_phi']\r\n",
        "  weights = ['mcWeight', 'XSection', 'scaleFactor_PILEUP', 'scaleFactor_ELE',\r\n",
        "             'scaleFactor_MUON', 'scaleFactor_LepTRIGGER', 'SumWeights']\r\n",
        "  all_vars = vars + weights\r\n",
        "  temp_tree = tree.arrays(all_vars, cuts)\r\n",
        "\r\n",
        "  # Define branches of variable length to be flattened\r\n",
        "  expand_vars = {'jet_pt': 0, \r\n",
        "                 'jet_phi': 0, \r\n",
        "                 'jet_eta': 0, \r\n",
        "                 'largeRjet_pt': 0, \r\n",
        "                 'largeRjet_phi': 0, \r\n",
        "                 'largeRjet_eta': 0,\r\n",
        "                 'lep_pt': 0,\r\n",
        "                 'lep_phi': 0,\r\n",
        "                 'lep_eta': 0 \r\n",
        "                 }\r\n",
        "\r\n",
        "  # Prepare dataframe columns\r\n",
        "  columns = []\r\n",
        "  temp_df = pd.DataFrame(columns=vars)\r\n",
        "\r\n",
        "  # Fill dataframe\r\n",
        "  for index, entry in progressbar.progressbar(enumerate(temp_tree)):\r\n",
        "    for var in vars:\r\n",
        "      if var in expand_vars:\r\n",
        "        temp_df.at[index, var] = float(entry[var][expand_vars[var]])\r\n",
        "      else:\r\n",
        "        temp_df.at[index, var] = float(entry[var])\r\n",
        "    temp_weight = 1.0\r\n",
        "    for weight in weights:\r\n",
        "      if weight != 'SumWeights':\r\n",
        "        temp_weight *= entry[weight]\r\n",
        "      else:\r\n",
        "        temp_weight /= entry[weight]\r\n",
        "    if len(temp_tree) > events_limit:\r\n",
        "      temp_weight *= (len(temp_tree)/events_limit)\r\n",
        "    temp_df.at[index, 'weight'] = temp_weight\r\n",
        "    if index == events_limit:\r\n",
        "      break\r\n",
        "\r\n",
        "  for column in temp_df.columns:\r\n",
        "    temp_df[column] = pd.to_numeric(temp_df[column])\r\n",
        "\r\n",
        "  return temp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuD5-gO_kt11"
      },
      "source": [
        "def make_plot( sig, bkg, variable, xlow=0, xhigh=1E6, bins=50 ):\r\n",
        "\r\n",
        "  np_bins = np.linspace(xlow, xhigh, num=bins+1)\r\n",
        "  plt.gcf().clear()\r\n",
        "\r\n",
        "  plt.hist( sig[variable], bins=np_bins, density=True, label='Sig', histtype='step' )\r\n",
        "  plt.hist( bkg[variable], bins=np_bins, density=True, label='Bkg', histtype='step' )\r\n",
        "  plt.xlabel(variable)\r\n",
        "  plt.legend()\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UvGCP_FLCls"
      },
      "source": [
        "def calculate_mt(df):\r\n",
        "\r\n",
        "  df['dphi_l1_met'] = np.abs(df['lep_phi'] - df['met_phi'])\r\n",
        "  df['mt'] = np.sqrt(2 * df['lep_pt'] * df['met_et'] * (1-np.cos(df['dphi_l1_met'])))\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7oOLrd2NzBo"
      },
      "source": [
        "The input ROOT files have a mixture of data-types inside them, namely floats and vectors of mixed length. However, our DNN requires a 'flat' input, so we must flatten these vectors. The function prepare_dataset performs this iteratively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cLkIXHVoXgC"
      },
      "source": [
        "stop_df = prepare_dataset(stop_tree)\r\n",
        "top_df = prepare_dataset(top_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMPKzUtKOFm5"
      },
      "source": [
        "In addition to the low-level inputs from the ROOT files, we can construct complex variables, such as the transverse mass. The function calculate_mt adds the transverse mass (mt) variable to the dataframes for both signal and background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9nH7ukmNfYB"
      },
      "source": [
        "stop_df = calculate_mt(stop_df)\r\n",
        "top_df = calculate_mt(top_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVNQg9iHOYUe"
      },
      "source": [
        "It would be nice to see some plots of these variables! The function make_plot enables this using matplotlib. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnYmSlyFkjX6"
      },
      "source": [
        "make_plot(stop_df, top_df, 'met_et')\r\n",
        "make_plot(stop_df, top_df, 'met_phi', xlow=-4, xhigh=4, bins=8)\r\n",
        "\r\n",
        "make_plot(stop_df, top_df, 'jet_pt')\r\n",
        "make_plot(stop_df, top_df, 'jet_phi', xlow=-4, xhigh=4, bins=8)\r\n",
        "make_plot(stop_df, top_df, 'jet_eta', xlow=0, xhigh=4, bins=8)\r\n",
        "make_plot(stop_df, top_df, 'lep_pt')\r\n",
        "make_plot(stop_df, top_df, 'lep_phi', xlow=-4, xhigh=4, bins=8)\r\n",
        "make_plot(stop_df, top_df, 'lep_eta', xlow=0, xhigh=4, bins=8)\r\n",
        "\r\n",
        "make_plot(stop_df, top_df, 'largeRjet_pt')\r\n",
        "make_plot(stop_df, top_df, 'largeRjet_phi', xlow=-4, xhigh=4, bins=8)\r\n",
        "make_plot(stop_df, top_df, 'largeRjet_eta', xlow=0, xhigh=4, bins=8)\r\n",
        "\r\n",
        "make_plot(stop_df, top_df, 'dphi_l1_met', xlow=0, xhigh=4, bins=8)\r\n",
        "make_plot(stop_df, top_df, 'mt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_7JvcWwOnIV"
      },
      "source": [
        "### 3. Steps toward training a DNN.\r\n",
        "To perform fully-supervised training, we need to give each of our input classes a label. As we're doing a binary classification, i.e. a this vs that classification, we will give the background the label 0 and the signal the label 1.  \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09ym5OjV8h2"
      },
      "source": [
        "# Set labels for each class\r\n",
        "stop_df['class'] = 1\r\n",
        "top_df['class'] = 0\r\n",
        "# Merge inputs into one dataframe\r\n",
        "merge_df = pd.concat([stop_df, top_df])\r\n",
        "# Separate the dataframe into one containing the kinematics, and one containing the truth labels. \r\n",
        "X = merge_df.drop(['class', 'weight'], axis=1)\r\n",
        "y = merge_df[['class']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL6OPm6FPaUb"
      },
      "source": [
        "We now use train_test_split to produce independent, randomised samples of events for training and validation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08mR7a-BNhWK"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.3, random_state=42 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3VWMfryPqIg"
      },
      "source": [
        "The last preparatory step is to scale our features. We need our inputs to be of roughly the same size (order of magnitude), and thus we use a min-max scaler, such that all features are within 0 to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhtxSqoBViMo"
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0,1))\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_val = scaler.transform(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzpaPYeIQHLT"
      },
      "source": [
        "### Neural networks!\r\n",
        "\r\n",
        "Finally we are getting around to defining our model, which is a neural network. We will use a fully-connected, feed-forward NN. This means that every node in a given layer is connected to all of the nodes in the previous and subsequent layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpxdWEJmVoxq"
      },
      "source": [
        "def NNmodel(activation_function='relu',learningrate=0.01):\r\n",
        "    \r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(13, input_dim=X_train.shape[1], activation='relu', name='input_layer'))\r\n",
        "    model.add(Dropout(0.5))\r\n",
        "    model.add(Dense(13, activation=activation_function, name='hidden_layer_1'))\r\n",
        "    model.add(Dense(13, activation=activation_function, name='hidden_layer_2'))\r\n",
        "    model.add(Dense(1, activation='sigmoid', name='output_layer'))\r\n",
        "    #configure the optimiser. This is where the learning rate is configured\r\n",
        "    opt=Adam(learning_rate=learningrate)\r\n",
        "    # can now pass the optimizer we just defined into the model\r\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\r\n",
        "    return model,learningrate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff6jKCxYowVI"
      },
      "source": [
        "Our model is defined as having an input layer which has the same number of nodes as we do input features. Our output layer has a single output, which will be a number between 0 and 1, corresponding to the prediction of the model for the event. There are 2 hidden layers, which are layers between the input and output layers, and make the neural network a 'deep' neural network.    \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX6XQouebRLQ"
      },
      "source": [
        "#### Details of the activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNfE6b5QtUFK"
      },
      "source": [
        "For each neuron, the output it makes will be of the form Sum(Weight * x) + bias. But, how does one convert this into a more \"binary\" yes or no statement? So in essence this becomes a function\r\n",
        "$Y=f(∑wi∗xi+b)$. \r\n",
        "An initial guess would be a step function (where a neuron was either on/off only). But, this becomes problematic when you have several nodes in a network making decisions. A continuum solution is better, so that \"on/off\" is replaced with \"90% on\" etc. This is the role of the activation function, and there are several available. Generally the behaviour that is desired is that it is continuous (or at least piecewise continuous) and piecewise differentiable.\r\n",
        "\r\n",
        "Typically activation functions are plotted on the range [-1,1], and we will discuss a few of these.\r\n",
        "\r\n",
        "One of the simplest classes of activation function is the linear function  $$Y=cx$$  (c non-zero, real), but in such a case, a network consisting of N layers with such a function can be compressed into a single layer with a different constant defined in the activation function. In order to investigate a network with a non-reducible structure, a\r\n",
        "\r\n",
        "A more useful case to binary classification (which has a similar behaviour to the step function) is the sigmoid, defined as:\r\n",
        "\r\n",
        "$$Y=sigmoid(x)=\\frac{1}{1+exp(−x)}$$ \r\n",
        "However, the primary issue of the sigmoid function is that for large values of x, the gradient of the activation function is small, which means that the neurons will learn slowly (since the learning process involves minimisation) for large values of x, which is not ideal.\r\n",
        "\r\n",
        "An alternative to the sigmoid is the tanh(x) function defined on range [-1,1].\r\n",
        "\r\n",
        "$$Y=tanh(x)$$ \r\n",
        "\r\n",
        "which is purely a re-scaled sigmoid, when one considers the hyperbolic identity\r\n",
        "$$tanh(u)=2 sigmoid(2u)−1 $$\r\n",
        "This function is steeper than the sigmoid across its range of definition, which may improve the speed of learning.\r\n",
        "\r\n",
        "Instead of using the linear activation function, another popular choice of activation function is what is known as \"Rectified Linear Units\" (ReLU), namely the function:\r\n",
        "$$Y=max(0,x).$$ \r\n",
        "Since for nodes with  $x < 0$ , the neuron is not activated (and is effectively discarded), it can greatly reduce computational time by only activating a small subset of nodes (this behaviour is known as sparse activation).\r\n",
        "\r\n",
        "However, for nodes with  $x < 0$ , these nodes are effectively \"dead\" and weights associated to these nodes cannot be updated/cannot \"learn\".\r\n",
        "\r\n",
        "Some attempts to rectify this include leaky ReLU(RelU), typically defined as:\r\n",
        "Y=max(ax,x),a=0.01 \r\n",
        "and exponential linear units (see https://arxiv.org/pdf/1511.07289.pdf)\r\n",
        "\r\n",
        "We will now plot some of these activation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW99LwAktToy"
      },
      "source": [
        "# typically these will be in tensorflow libraries by default\r\n",
        "import numpy as np\r\n",
        "def sigmoid(x):\r\n",
        "  sigmoid=1/(1+np.exp(-x))\r\n",
        "  return sigmoid\r\n",
        "\r\n",
        "def ReLU(x):\r\n",
        "  if(x>0):\r\n",
        "    return x\r\n",
        "  else:\r\n",
        "    return 0\r\n",
        "\r\n",
        "\r\n",
        "def LeakyReLU(x):\r\n",
        "  if(x>0):\r\n",
        "    return x\r\n",
        "  else:\r\n",
        "    return 0.05*x # effect exaggerated for plot\r\n",
        "\r\n",
        "def PlotActivations():\r\n",
        "  #function to show the activation functions discussed above\r\n",
        "  \r\n",
        "  range_array=np.arange(-10.0,10.0,0.01)\r\n",
        "  y_sigmoid=[]\r\n",
        "  y_tanh=[]\r\n",
        "  y_RelU=[]\r\n",
        "  y_LRelU=[]\r\n",
        "  for x in range_array:\r\n",
        "    y_sigmoid.append(sigmoid(x))\r\n",
        "    y_tanh.append(np.tanh(x))\r\n",
        "    y_RelU.append(ReLU(x))\r\n",
        "    y_LRelU.append(LeakyReLU(x))\r\n",
        "\r\n",
        "  plt.plot(range_array,y_sigmoid,label=\"sigmoid\")\r\n",
        "  plt.plot(range_array,y_tanh,label='tanh')\r\n",
        "  plt.plot(range_array,y_RelU,label='relu')\r\n",
        "  plt.plot(range_array,y_LRelU,label=\"leaky relu\")\r\n",
        "  plt.xlabel('x')\r\n",
        "  plt.ylabel('Y')\r\n",
        "  plt.legend()\r\n",
        "  return\r\n",
        "PlotActivations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX-jDAtQtcDE"
      },
      "source": [
        "Tensorflow (Keras backend) has several of these activation functions as built-in modules configured by string. The list of available pre-made functions can be found here https://www.tensorflow.org/api_docs/python/tf/keras/activations . Custom configurations are beyond the scope of this tutorial, but documentation can be found here: https://keras.io/api/layers/activations/ \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGDmn0PEt4Di"
      },
      "source": [
        "As you can see, the effect of using ReLU for the hidden layers is that the NN learns faster than that of the NN configured with a sigmoid in the activation function, taking advantage of the sparse activation discussed above, with similar overall performance.\r\n",
        "\r\n",
        "In a real physics use case on large datasets, the CPU/GPU processing time is a major overhead that one must consider when implementing a NN, so fewer epochs to converge = faster.\r\n",
        "\r\n",
        "However, there is no preferred choice of activation functions in a NN, so one must attempt with trial and error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcjUchL0bXXV"
      },
      "source": [
        "#### Training our model\r\n",
        "We create an instance of our model so that we can train and test it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOWQfBRoVq3r"
      },
      "source": [
        "model,_ = NNmodel()\r\n",
        "print( model.summary() )\r\n",
        "plot_model( model )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8miA737RcMo0"
      },
      "source": [
        "When training our model, we wish to avoid overtraining - where we specialise our model too much on the training data, and thus lose any discrimination on unseen data. We use early stopping as one way of overcoming this. Early stopping halts the training once one of the training metrics stops improving, in this case the value of the loss function evaluated using the validation data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru_RAuvoVxXH"
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\r\n",
        "history = model.fit( X_train, y_train, epochs=200, batch_size=10, \r\n",
        "                    validation_data=(X_val, y_val), verbose=2, \r\n",
        "                    callbacks=[early_stop] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5hXM3cLPqfv"
      },
      "source": [
        "We should take a look at some validation plots to ensure the model does not behave much differently for the training and validation sets - this could be a sign of overfitting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca0ZjID9Xbd7"
      },
      "source": [
        "doValidationPlots = True\r\n",
        "if doValidationPlots:\r\n",
        "\r\n",
        "  _, training_accuracy = model.evaluate(X_train, y_train)\r\n",
        "  _, validation_accuracy = model.evaluate(X_val, y_val)\r\n",
        "\r\n",
        "  plt.plot(history.history['loss'], label='Training sample')\r\n",
        "  plt.plot(history.history['val_loss'], label='Validation sample')\r\n",
        "  plt.xlabel('Training epoch')\r\n",
        "  plt.ylabel('Loss')\r\n",
        "  plt.legend()\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "  plt.plot(history.history['accuracy'], label='Training sample ({}%)'.format(round(training_accuracy*100, 1)))\r\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation sample ({}%)'.format(round(validation_accuracy*100, 1)))\r\n",
        "  plt.xlabel('Training epoch')\r\n",
        "  plt.ylabel('Accuracy')\r\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP8gnMsyQnW8"
      },
      "source": [
        "### 4. Evaluating the performance of our model.\r\n",
        "At the end of the analysis chain, we want to test if our DNN gives us discriminatory power. We do this by using the model we trained to predict the class of every event in our samples:\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbkWUA8Qn7D8"
      },
      "source": [
        "do_predictions = True\r\n",
        "if do_predictions:\r\n",
        "  bins = np.linspace(0,1,num=51)\r\n",
        "\r\n",
        "  sig_df = merge_df[merge_df['class'] == 1].copy()\r\n",
        "  bkg_df = merge_df[merge_df['class'] == 0].copy()\r\n",
        "  sig_weights = sig_df['weight']\r\n",
        "  bkg_weights = bkg_df['weight']\r\n",
        "\r\n",
        "  sig_true = scaler.transform(sig_df.drop(['class', 'weight'], axis=1))\r\n",
        "  bkg_true = scaler.transform(bkg_df.drop(['class', 'weight'], axis=1))\r\n",
        "\r\n",
        "  sig_preds = model.predict(sig_true)\r\n",
        "  bkg_preds = model.predict(bkg_true)\r\n",
        "\r\n",
        "  sig_df['preds'] = sig_preds\r\n",
        "  bkg_df['preds'] = bkg_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bnjYtVXufLQ"
      },
      "source": [
        "We now make a plot of these predictions for each class!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1McIrMKp-ysZ"
      },
      "source": [
        "lumi = 139000\r\n",
        "\r\n",
        "plt.hist( sig_preds, bins=bins, alpha=0.5, label='Signal', weights=lumi*sig_weights )\r\n",
        "plt.hist( bkg_preds, bins=bins, alpha=0.5, label='Background', weights=lumi*bkg_weights )\r\n",
        "plt.yscale('log')\r\n",
        "plt.xlabel('NN output')\r\n",
        "plt.ylabel('Events / bin')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7pw9029uj8y"
      },
      "source": [
        "As we **should** see, the model peaks at low NN output for the background class and at high NN output for the signal class, as we hoped! We can now make a crude estimate of the discriminatory power of our classifier by placing a cut on the output of the NN, and counting the number of signal and background events passing this cut (cut & count). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFzkpGNkAQAU"
      },
      "source": [
        "cut = 0.95\r\n",
        "\r\n",
        "signal_yield = lumi * sig_df[sig_df['preds'] > 0.95]['weight'].sum()\r\n",
        "background_yield = lumi * bkg_df[bkg_df['preds'] > 0.95]['weight'].sum()\r\n",
        "\r\n",
        "print('Signal events: {}'.format(signal_yield))\r\n",
        "print('Background events: {}'.format(background_yield))\r\n",
        "print('Significance: {}'.format(signal_yield/np.sqrt(background_yield)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQAuh6GVvDf0"
      },
      "source": [
        "In principle, this is an example of an analysis chain using a DNN as the main discriminant. We can now play around with the structure of our DNN and alter the learning rate. In addition, we can change the loose selections applied at the start of the chain and re-run!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6SmUymdQSO_"
      },
      "source": [
        "## Exercise: Varying number of hidden layers\n",
        "Try adding and removing hidden layers to see if the *performance* of your network changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTc6sd9xPdha"
      },
      "source": [
        "## Exercise: Evaluating the impact of learning rate on the NN training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ketm_EGAuBBr"
      },
      "source": [
        "Useful parameters to configure: Learning rate\r\n",
        "NNs involve configuration of the learning rate (extent to which weights are updated after each back-propagation step/epoch), which is typically taken to be in the range  (0.0,1.0) . The learning rate determines the speed at which the model \"learns\", such that the larger the learning rate, the faster the model learns at the cost of a somewhat sub-optimal set of parameters and the converse case for the small learning rate.\r\n",
        "\r\n",
        "However, if the learning rates are too large, the weight updates will be too large and will oscillate over training epochs. Conversely if the learning rates are too small, a model may converge to a sub-optimal set of weights.\r\n",
        "\r\n",
        "The Tensorflow default for this parameter is LR=0.01, and we will vary this parameter to LR=0.02 to demonstrate the impact of this parameter.\r\n",
        "\r\n",
        "The exact choice for your use case must be determined by trial and error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO2Xk-me9DgA"
      },
      "source": [
        "Given the default model considered, please evaluate the impact of varying the learning rates on the training.\n",
        "\n",
        "Sample plotting code is added here for the respective plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bSTg8qb9l9R"
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "# fit our model with the alternative learning rate\n",
        "NNmodel_altLR,learningrate=NNmodel(learningrate=0.02)\n",
        "history_altLR = NNmodel_altLR.fit( X_train, y_train, epochs=200, batch_size=10, validation_data=(X_val, y_val), verbose=2, callbacks=[early_stop] )\n",
        "#copy the default fit history from the default NN model\n",
        "history_reference=history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S37lGDXK_39j"
      },
      "source": [
        "plt.plot(history_reference.history['loss'],label='loss (nominal LR)')\n",
        "plt.plot(history_altLR.history['loss'],label='loss (LR={})'.format(learningrate))\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Training loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwGTuOAcuH0U"
      },
      "source": [
        "plt.plot(history_altLR.history[\"accuracy\"],label='accuracy (LR={})'.format(learningrate))\r\n",
        "plt.plot(history_reference.history['accuracy'],label='accuracy (nominal LR)')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upLXj5aJAB80"
      },
      "source": [
        "Similarly, try to see how this affects the output classification (if at all!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di-wiIBmQEwa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skQlh5wTBImw"
      },
      "source": [
        "##Exercise: Impact of activation functions on the NN training\n",
        "Change the activation functions in the hidden layers to `'sigmoid'` and compare your results to the default NN configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVHXG4bwbiS-"
      },
      "source": [
        "model_sigmoid,_=NNmodel(activation_function='relu')\r\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\r\n",
        "history_sigmoid = model_sigmoid.fit( X_train, y_train, epochs=200, batch_size=10, \r\n",
        "                                    validation_data=(X_val, y_val), \r\n",
        "                                    verbose=2, callbacks=[early_stop] )\r\n",
        "#copy reference history again\r\n",
        "history_reference=history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_3nCZTvtj98"
      },
      "source": [
        "plt.plot(history_sigmoid.history['loss'], label='train_loss_sigmoid')\r\n",
        "plt.plot(history_reference.history['loss'], label='train_loss_nominal')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCRE2DkotuQb"
      },
      "source": [
        "plt.plot(history_sigmoid.history[\"val_loss\"],label='test_sigmoid')\r\n",
        "plt.plot(history_reference.history[\"val_loss\"],label='test_nominal')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.ylabel('Validation loss')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXU14wZ-tyd7"
      },
      "source": [
        "plt.plot(history_sigmoid.history['accuracy'],label='acc_sigmoid')\r\n",
        "plt.plot(history_reference.history['accuracy'],label='acc_nominal')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQY2QnPEKsG5"
      },
      "source": [
        "Extension: Do the same exercise again, but with the sigmoid activation function replaced in the output layer with the $tanh$ activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABGt7YoqPkC2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}